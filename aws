use lauch configuration/template, to update give new launch config/template,,free price for backend servers, iam on asg work for backen, if ec2 get temnited fr any reaon
ASG will recreate, replace instance marked unhealthy by LB. 
Scaling policies: 2 types 1>Dynamic 2>Predective: 1>Dynamic scaling: 1.1> Target tracking: if avg cpu > 40% increase 1.2>simple/step :can create own cloudwatch alarm and 
rules like if cpu >80 add 2 ec2 and cpu < 60 remove 2 instnaces can define cloud watch metrics 1.3>schedules:known usage 2>Provisioning:forecast & analysis by asg
ASG cooldown period:after scaling activity cooldown will come, no launch and terminate in this to stabilize metrics,,def 300sec, low cool hig perf, use golden ami for dis
ASG termination policy: default: find az with n number of instance, and inside it delete the oldest version means first created, balance instance in az,s along versions
ASG lifecycle hooks: p.wait,p.proceed,t.wait,t.proceed,, launch  config vs template: template is old not reusable need recreate, config: partial reusable, ondeamand or
spor or both, recomnded by aws,, both allows ami in confiuration
RDS: Relational db service : svc manage by aws for db, use sql, types: postgresql, mariadb, mysql, oracle,microsoft sql, use: auto provison,patching,bckp and restoere
point of time recovery, monitering,read replica, multi az, maintainance wid,auto scal vertical and horizontal,backedebs gp2/3,cant ssh
Automated backup: daily full bkp in maintainace by aws, transacton logs 5 mins, point oftime recovery(old to 5 mins),7 days retentionsnap shot: manually by user, alive
until delete,USE:backup specific time,RDS auto scaling: for max storage thresold, rules:storage<10%,low storage for 5mins,no transaction in 6 hours,unexpcted load,all dbs
Read replicas: 5, for read scale,async eventually consist, update app with all read replica urls, for selectquery only,use: to read db for analysis reporting tools.pay fr
cross region replicasntwrk no pay fr sameregion,RDS multiaz:DR,stand by in sameregion,write same time,same dns no trouble forapp, autofailover nt scaling,read replicas
can set as this,how multiaz work:snaptaken,restore in new az,enable sync..RDS encryption: both master and RR with KMS-AES256,at launchtime, if master not RR not encrypt,
another way TDE(transperant data encrypt),in flight by ssl,force users for this,postgres(rds_force_ssl=1),my sql update with in db, how to encrypt unencrypted rr- restore
db from it and enableencrypt at launch, SECURITY: ntwrk sec: put in pvt sbnt, iam policy for users managing db,, IAM authentication by iam and rds api calls, sql and 
postgres only,auth token 15 mins, 

AURORA: not open source, postgress and mysql, 5x on mysql and 3x on postgresof rds, storege autoscale in 10gb and max 128tb, 15 replicas, 20%> rds price,
self healing apps peer to peer if something bad, storge stripped into small pieces, one master for writes and remain reads, master + 15 rr in multi az, write has 
writer end point url and reader has reader end point only one for all rr actsa as LB,if master fails one of rr turn into master in 30 sec with same writer end point,
reagonal cluster= write + read endpoint sec smae as rds,, Features: 1>autoscaling: aurora can monitor read req and increase replicas.2>custom end point:some subset of rr
for analytical quaries, if this avail no use for read endpoint 3>Serverless:automated installation, scaling and pay per sec based on usage,cost effecive, no capaity plan
generate proxy by aws to connect to aurora, 4>Multi master: immediate failover,every node has read and write asno promote rr as master, 5> Auroraglobaldb: cross 
regionreplica, 1 primery region and 5 secondary regions, 16 rr across these, latency < 1 sec, rto9recovery time object at DR)< 1 min, for global applications
6>Machine learning:mlbased predicton, integration b/w aurora and aws ml sevces, support: amzon sega maker and comprehend use: fraud detect, prdct recomndtion,ads trgeting

Elastic cache: redis&mem-cached, in memory db, highperf, lowlatency, reduce to db for common quaries, app ned heavy changes, aws can manage this fully for operations
redis: multi az/rr/persistance/backup&restore Memcached: multi node(sharding),multi threaded, nopersistant,restore & backup
Securiyt: IAM roles for only creae and delete cache not for the data inside cache, Redis auth: user/pwd while creating db will used inside app, ssl, memcached-sasl auth
types of caching: 1>lazy: some stale data,2> Write through: when db updates cache update no stale data3>session store: session data using ttl:: USE_CASE: leader boards
in gaming sites as redis sorted set : unique and element ordering new order for new element:PORTS:postgres:5432/mysql:3306/oracle:1521/Mssql:1433/Maria:3306/aurora:5432

DNS: domain name system:convert host name into backend ip, terminology: domain registrer (ca), domain record:a,aaaa,cname,alias zonefile: dns record ,name server:quaries
(authorative/non autherative)-TLD-.com.in.us.gov,SLD-second amazon.com, 
flow--> web server --> local dns by org/cache iv ip to --> root dns give ip to--> TLD --> SLD --> here it will send the browser the actual ip of the service

Route 53: managed,scalable,authorative(user can maintain dns recordds), is domain register gie certs, health check of resources,100% availability sla,port 53 for dns
RECORD: contains domain or sub domain name/request type (A or AAAA or  CNAME or Alias)/value/Routing policy/TTL,, 
A: Maps a host name to ipv4,,AAAA: maps ahost name toipv6,,CNAME: maps a host name to another host name and dest ost name is eiter a or aaaa record,, and can't be a 
root domain ALIAS:is same as root domain only but can also use root,,,Alias: work for root and non root, health check,auto recginise backend ip changes, suported 
backends: elb, cloudfront,api,ebs,s3,vpe end points,oter r53 record in same hosted zone,global accelerator
NS(naming serer):for hosted zone contain dns name and ip can respond to quaies for trafic route
Hosted Zone: contain record that how to rout traffic,2 public (internet)and private(vpc), $0.50 per month
**Nslookup ordig cmds for lburl tosee the backend ips, TTL: time to leave, attach in client responce along public ip,2 high,low..High: less trafic for r53 but old data
Les: More traffic for r53 less ooutdated,must for all record type not ALIAS, dig command to see ttl
ROTING POLICY: simple,weighted,failover,latency based,geolocation,geo proximity,multi-value,, SIMPLE: tosingleresource, multiple values can,send to client, best chosen
by client, Weighted: control % trafic to speciic resource,backend dns have same name and type, health checks,use: LB b/w regiosn, test new ap fr less trafic, weight 
zero to stop sending, or 0 for all to sent trafic equally,,LATENCY: route to low latency instances, b/w user and aws region,health cheks,FAIL OVER: like dr, 1 primary 
1 secondary, primary not go for secondary,based on health check,,GEO LOCATION: Depends on location of user, if user in us go this ip,if in german go thi,default record
use: content loca,restrict in country,load balance,, health checks,,GEO PROXIMITY: baesd on geographic location, biasing,less-low, high-more,, traffic flow must(UI)
Multi Value: routing to multiple resources/values, health check, 8 health record returned for each value, only healthy records, use dig to check ip's, 
Health Checks: only for public resources, 3 ways of monitering,1>metric 2>Health check3>cloud moitering,, health check will have their own metrics and integrate with 
cloud wathc metric, 15 health check check end point, threshold is 3, 30se reguler check,10 sec higher,support https/s,tcp pass code 2xx or 3xx or 1st 5120 bytes data
CAlculated health check: check multiple checks, user or,and,not operator,256chaild health check,define how many chaild health check, for private use cloudwath metric
3rd party domain: aws can maintain 3rd party domains, create hosted zone and add ns server to 3rd party, go dady will search for aws r53 by ns server
HA: Route 53 + ELB + ASG + multi AZ
Quick instance launching: Golden AMI/bootstraping/Golden AMI + boot straping(hybrid) can have rds db, esb..etc
EBS(elastic bean stalk)auto provisioning,load balancing,instance config.go,java,.net,php.etc

AWS S3: Infinay scaling storage, object stored in bucket, globaly unique name, **s3 is global level and bucket is regional level, key/value - path/file, max 5tb(5kgb)
if >5tb multi part upload, object have meta data,upto 10 tags,versioning,, how to open file in s3 2 from aws console open(presigned)/public access link,versioning at
bucket level, version come for new files, for old files version is null, use: unintended delte,rollback,, suspending version dont delete previous files,d/f version id 
for every upload,delete marker- file is not available but older versions are there,can delete this if needed,
S3-Encryption: SSE-S3/SSE-KMS/SSE-C/Client side> 1>SSE-S3:serverside,aes-256,header x-amz-server-side-ecryption:aes-256,key own by AWS,2>SSE-KMS: server side, key
maintained by kms,header sert to awskms,use:user control + audit on key 3>SSE-C:key maintain by user,sent in https header for all request,same key for decrypt, https 
traffic to s3 4>Client side encruption: user encrypt/decrypt data before s3,, Encypt intranstion:s3 have 2 ends http/https any thing can ue: https sure for sse-C

S3 security: user based: IAM policy to control api calls on s3, resource based: bucket side rules,define principles,object and bucket acl (access control list)
if iam policy permission accept, reource policy allo itno explicit deny rule
s3 bucket policy:jsone;same format as iam policy, version,id,statement,sid,effect,principal,action,resources,use: grant public access,force object to encrypt at upload
grat access to other account,, logging and audit: s3 access logs can stord in other buckett, api calls can be logged in aws cloud trail,, user security: MFA delete, 
presigned url: temporary url for limited time,,for 403 check public access and policies
S3-CORS: (cross origin resource sharng)- getting resources fro different origin, to allows thos cors headers eg: access-control-allow-origin,, allow oother origins
IAM policy simulator to test the policy,, ec2 instance metada: http://169.254.169.254/latest/meta-data,,aws-sdk control aws from app,, mfa needed for delete and suspend
versioning, not for enable version, bucket owner or root can enable/disable MFA,,s3 encryption 2 ways: hw to force: s3 policy condition,default encryption(dont do for
already encrypted ones)--access logs: all requestt details accept,deny,action, athen to analyse,**never make logging bucket same bcome loop,,

S3 replication: CRR/SRR/different acnt, must versining, asynchronus copy, must properr iam perission,not retro active only new objet will replicate,for delete 2 options 
replicate/non replicate for malicious deletes,no chaining one to one relation,
S3 presigned url: sdk and cli, download(cli) upload (sdk and hard), 3600 sec r set --expires-in time,inherit permisn of user granting,use:temporar acess,
Storage classes: standers,stander-IA,onezone-IA,intelligent tiering,glacier,lacier deep archive,reduced redundancey storage(depricated)
1>S3 standerd:high durability,availability 99.99,sustainn 2 concurrent fails for DR, 2>S3 Standerd IA: less frequently,availability&durability-99.99,low cost,2 failure
3>s3 one zone ia: single az,low latency, support ssl, 20% cost < IA, use :store second bkp,thumbnails4>Intelligent tiering: monitering monthy,auto-tiering fee,auto_
matically moves,5> Glacier or vault: for archives/backups,long term 10 years, dur 99.99, per mnth $0.004/GB, stored in vaults,retrival 3 options: expidiated (1-5msn),
standerd (3 to 5),bulk(5 to 12 hrs),, min time 90 days,, 6>Glacier depp archives: more long time, more cheaper, retrival: standerd:12 hrs, bulk 48 hrs, ** for glacier 
we can't see them unless we retrive it, S3 Lfecycle: 1>transaction action: just move, move object to ia after 60days,move to glaicer after 6 months 2>Expiration actions
:for delete after some time use: log files delete, partial upload,old version,,rules can be applied specific path or object,,
S3 Anlytc: to help lifycycle transaction rules, not onezone-ia, report daily,24 to 48 fr 1st start,, multi upload, prefer >100mb, must >5gb, help parllel upload
S3 Tranfer acceleration: increase speed as file- aws edge near- s3 bucket,,s3 byte range fetch: can download small parts of download,speedup,to download some part file
S3 select & glacier select: retrive less data by perform serverside quaries, fast and quick,, S3 Event notifications: action at s3 like create, remove, relicated,
delete object.


